{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# import the necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Embedding, SpatialDropout1D, LSTM, GRU, Dense, Activation, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# import LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#Oversampling library\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "# Word2Vec\n",
    "from gensim.models import Word2Vec, KeyedVectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 135303\n",
      "Max sequence length: 957\n",
      "Padded sequence shape: (8675, 957)\n"
     ]
    }
   ],
   "source": [
    "#Download data \n",
    "df = pd.read_csv('cleaned_dataset.csv')\n",
    "\n",
    "# replacing ||| with space\n",
    "df[\"cleaned_posts\"] = df[\"cleaned_posts\"].str.replace(\n",
    "    r\"\\|\\|\\|\", \" \", regex=True\n",
    ")\n",
    "\n",
    "#Tockenizer\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(df['cleaned_posts'])\n",
    "tokenizer.word_index # Get our learned vocabulary\n",
    "word_index = tokenizer.word_index\n",
    "VOCAB_SIZE = len(word_index)+1 # Total words\n",
    "print('Vocabulary size:', VOCAB_SIZE)\n",
    "\n",
    "\n",
    "#Tockenize the words\n",
    "X = tokenizer.texts_to_sequences(df['cleaned_posts'])\n",
    "MAX_SEQ_LENGTH = max(len(seq) for seq in X)\n",
    "print('Max sequence length:', MAX_SEQ_LENGTH)\n",
    "\n",
    "#Pad the sequence\n",
    "X = tf.keras.preprocessing.sequence.pad_sequences(X, maxlen= MAX_SEQ_LENGTH, padding = 'post') # Pad the sequence to the same length to make it uniform\n",
    "print('Padded sequence shape:', X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: 300\n",
      "Shape of embedding matrix: (135303, 300)\n",
      "Embedding matrix[1]: [-0.03662109  0.01452637  0.03515625  0.23046875 -0.20800781  0.26171875\n",
      " -0.13183594 -0.08740234  0.07519531  0.03881836 -0.19726562 -0.37109375\n",
      " -0.22460938 -0.05029297  0.14648438  0.08398438 -0.0625      0.3828125\n",
      "  0.05664062 -0.09277344 -0.20898438  0.11035156  0.36132812  0.28710938\n",
      " -0.15332031 -0.16113281 -0.3828125  -0.05395508 -0.140625   -0.29101562\n",
      "  0.18261719  0.09326172 -0.19628906 -0.00500488 -0.07910156  0.296875\n",
      " -0.38085938  0.44335938  0.3671875   0.20117188  0.07568359 -0.25585938\n",
      "  0.1953125   0.10253906  0.23730469  0.00772095  0.1875     -0.20117188\n",
      " -0.09277344  0.10107422  0.0246582   0.18457031  0.19824219  0.19140625\n",
      " -0.05419922  0.13476562  0.00506592  0.10644531 -0.05322266 -0.18945312\n",
      " -0.10498047 -0.01611328 -0.26171875  0.05004883 -0.04882812 -0.3046875\n",
      " -0.00799561 -0.14257812 -0.359375    0.3671875   0.10546875  0.40234375\n",
      "  0.11035156  0.08740234 -0.32226562 -0.05834961  0.16796875 -0.04272461\n",
      "  0.19042969  0.12158203 -0.07324219 -0.08886719 -0.02087402 -0.20800781\n",
      "  0.08349609  0.29492188  0.00698853  0.24902344  0.02929688 -0.06738281\n",
      " -0.10400391  0.06835938  0.1796875  -0.15136719 -0.23535156  0.140625\n",
      " -0.14648438 -0.01647949  0.41601562 -0.05664062 -0.05981445  0.125\n",
      "  0.11572266  0.21777344 -0.10742188  0.17773438  0.01318359  0.15722656\n",
      "  0.06347656 -0.14160156 -0.1328125  -0.34765625  0.09130859 -0.00311279\n",
      " -0.15136719 -0.01190186 -0.15820312 -0.15039062  0.1796875  -0.2421875\n",
      "  0.19238281  0.04199219 -0.13671875  0.13769531  0.42578125 -0.05810547\n",
      " -0.2890625   0.14355469 -0.04443359 -0.11621094 -0.31640625  0.06103516\n",
      " -0.02783203 -0.12890625 -0.33007812  0.23632812 -0.19921875 -0.0291748\n",
      "  0.05444336  0.08886719  0.3828125   0.04174805  0.24316406  0.03039551\n",
      " -0.359375    0.02563477 -0.03198242  0.02111816  0.10058594 -0.19921875\n",
      "  0.02893066 -0.13671875 -0.09228516 -0.07910156 -0.26171875 -0.14257812\n",
      "  0.29296875  0.26953125 -0.16113281  0.16503906  0.03112793  0.13574219\n",
      " -0.00102234  0.09814453  0.06201172 -0.25390625  0.05444336  0.109375\n",
      " -0.11962891  0.28320312 -0.31640625 -0.17089844 -0.00445557 -0.06591797\n",
      " -0.06396484  0.11376953  0.22167969 -0.18066406  0.3984375  -0.06542969\n",
      "  0.06298828  0.01831055  0.05932617  0.03955078  0.3359375   0.25390625\n",
      " -0.17382812  0.2421875  -0.02746582 -0.10351562  0.203125   -0.01831055\n",
      " -0.06787109  0.11621094  0.23828125  0.23925781 -0.07373047 -0.0050354\n",
      " -0.02185059 -0.19628906 -0.33398438 -0.13964844  0.01733398 -0.24804688\n",
      " -0.14355469 -0.0255127  -0.16015625 -0.13378906 -0.328125    0.37109375\n",
      "  0.20996094 -0.09912109 -0.328125   -0.11230469 -0.19238281 -0.17089844\n",
      "  0.28515625 -0.18554688  0.01745605  0.10253906 -0.31054688 -0.06591797\n",
      "  0.11621094  0.14746094  0.17382812 -0.19042969  0.08007812  0.01080322\n",
      " -0.296875   -0.14550781  0.18066406 -0.14941406 -0.1640625  -0.21289062\n",
      "  0.07373047  0.05053711  0.00349426  0.05932617 -0.02099609  0.13183594\n",
      "  0.28710938  0.23730469  0.06445312 -0.04711914 -0.34570312  0.05102539\n",
      " -0.10058594  0.4453125  -0.12597656 -0.17675781  0.05664062  0.08642578\n",
      "  0.04077148  0.29296875  0.33398438 -0.06884766  0.19238281  0.0859375\n",
      "  0.01672363  0.15722656  0.00982666 -0.00106812  0.18554688  0.06445312\n",
      " -0.05981445  0.18945312 -0.11230469 -0.18359375 -0.05688477 -0.27148438\n",
      " -0.00787354 -0.13867188  0.03759766  0.28710938  0.13574219 -0.10498047\n",
      " -0.15234375 -0.20996094 -0.18066406  0.13867188  0.07226562  0.16308594\n",
      "  0.11962891  0.04345703 -0.01977539  0.31054688  0.13476562 -0.16601562\n",
      "  0.03198242 -0.0324707   0.04370117  0.16113281 -0.05517578  0.00921631\n",
      " -0.24316406  0.04760742 -0.27734375 -0.12011719  0.05029297  0.140625  ]\n"
     ]
    }
   ],
   "source": [
    "# Word 2 vec model \n",
    "gn_vec_zip_path =\"/Users/salmazainana/Downloads/GoogleNews-vectors-negative300.bin\"\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(gn_vec_zip_path, binary=True)\n",
    "\n",
    "# Create an embedding matrix\n",
    "embedding_size = 300\n",
    "print('Embedding size:', embedding_size)\n",
    "\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, embedding_size))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in word2vec_model:  # Use word2vec_model directly instead of word2vec_model.wv\n",
    "        embedding_vector = word2vec_model[word]  # Same here\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Check the shape of the embedding matrix\n",
    "print('Shape of embedding matrix:', embedding_matrix.shape)\n",
    "\n",
    "# Check the embedding matrix\n",
    "print('Embedding matrix[1]:', embedding_matrix[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Learning variables\n",
    "LEARNING_RATE = 0.01\n",
    "DROPOUT = 0.1\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 957, 300)          40590900  \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 300)               721200    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 301       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 41312401 (157.59 MB)\n",
      "Trainable params: 41312401 (157.59 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Build Model\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    Embedding(\n",
    "        VOCAB_SIZE,\n",
    "        embedding_size, #input vector size\n",
    "        input_length=MAX_SEQ_LENGTH, #input length\n",
    "        weights=[embedding_matrix],\n",
    "        mask_zero=True,\n",
    "        trainable=True,\n",
    "    )\n",
    ")\n",
    "# model.add(SimpleRNN(EMBEDDING_VECTOR_LENGTH, dropout=DROPOUT, recurrent_dropout=DROPOUT, activation='sigmoid', kernel_initializer='zeros'))\n",
    "# model.add(GRU(EMBEDDING_VECTOR_LENGTH, dropout=DROPOUT, recurrent_dropout=DROPOUT, activation='sigmoid', kernel_initializer='zeros'))\n",
    "model.add(\n",
    "    LSTM(\n",
    "        embedding_size,\n",
    "        dropout=DROPOUT,\n",
    "        recurrent_dropout=DROPOUT,\n",
    "        activation=\"sigmoid\",\n",
    "        kernel_initializer=\"zeros\",\n",
    "    )\n",
    ")\n",
    "# model.add(Bidirectional(LSTM(EMBEDDING_VECTOR_LENGTH, dropout=DROPOUT, recurrent_dropout=DROPOUT, activation='sigmoid', kernel_initializer='zeros')))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "optimizer = tf.keras.optimizers.Adam(lr=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\", optimizer= optimizer, metrics=[\"accuracy\"]\n",
    ")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['type']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled dataset shape Counter({'INFJ': 1462, 'ENFP': 1462, 'ENTP': 1462, 'INFP': 1462, 'ENFJ': 1462, 'INTJ': 1462, 'ENTJ': 1462, 'INTP': 1462, 'ESFJ': 1462, 'ISFP': 1462, 'ISTP': 1462, 'ISFJ': 1462, 'ESTJ': 1462, 'ESTP': 1462, 'ISTJ': 1462, 'ESFP': 1462})\n",
      "example of resampled dataset [  508   622    25  7602 47454    51  1258   174   343   112   435   495\n",
      "   722   476  6584 16338  6407 90931 36784   233     9    23  1072  3544\n",
      "     5     9    15   600     9  1030   175    21     3   159    17  3753\n",
      "  9002  1012   431   385    72   830     2   191   350  2292   975   509\n",
      "  4434   509   133   509   762  1973  1101  1483   945  3475  6141  2815\n",
      " 14995   714  9817   225  2656   372   540   649  1076 13263 21424  1869\n",
      "   192   520 90932    36    84   529   491  1212   590   133   136   419\n",
      "  2679   708   425  3653     6  1084   396   945  3272     2  3164    28\n",
      "    18  1534  1870  5984    71     8     3   927     6   656  1534   397\n",
      "   174    38    91  1610  1667    87   111   874    17    38     1   174\n",
      "    18   579    10     9     2   511 12701    97   831  1998   831   559\n",
      "  4715  1990    90  4534  2338  1808  3215   193    60  1278  3770  1205\n",
      "  1208 45382 41070   298    83     8    25   142  2931  5420  5949   408\n",
      "  1107   839  4482     2   556  3509  4482   612 90933  5420   115   479\n",
      "  3365   515   544   188  1447   775    17     1   614   235   225   877\n",
      "   135   300    26   614  2054  2054 90934  4383  1100   204   177  2942\n",
      "    26   378   349  1103 11993   100    18  1103   318  1375  6978  1967\n",
      "    81    61  1785  1460  4000  1568   125    74    11    38     9   975\n",
      "    29  1287  4491   403   122  1736  1690   338  1736  1007   387  1007\n",
      "  5539  9235  4491   127    25   572    17    14    96    99    30    38\n",
      "     1    73    56   440  1070   189   525   755    29     6  3894   207\n",
      "    14     2     1    20   152     8    26  1320  3793  2065     3     7\n",
      "     1    96     1 12235  3548   156    30   455    58   123   125    40\n",
      "   224     1 11927  2037  8619   934 13238  1015   276  5257   701   791\n",
      "   107   392    10  1228    10    90   132     5  1089   107  1490    23\n",
      "   107    90   127   107    14    32    22  7071  3402   476     6   252\n",
      "    69     8     2   563  3402    24     1    49     1    47    45   105\n",
      "   332     1    74    72   101   391   117  3280   602  1550    63   998\n",
      "   118    27     4     5  2910  9209  1308  2869   132   375  9795   310\n",
      "   821   125    81   815   471     3     4  4350  3375   171   872  1170\n",
      "    80  3371    86   872  2122     5   552    58   171   182    26   308\n",
      "   896    38     4   754   252    31  1054   110   886   779   771    76\n",
      "  2653   298    25   717   282   554     2   248   508  2650   206  1977\n",
      "   206     2  2664  1433   166    20   365   120  2249   666  2897   647\n",
      "   598   130  1088  3101     1 90935    40   217    60   568   479    60\n",
      "   125   106   165 11408    22  4090  1707    11   476  1779   138   373\n",
      "  1901     6    13   262    69  5113   769   493   234     6  2016  4205\n",
      "    67    70   491  1196    33   259   493   686   153 10793   520   740\n",
      "  1473    63    17   196   103   514   100   103   771   660    14   514\n",
      "   156   346   383    32     2     6    20  1473    26   520     2 17018\n",
      "  3216   867   195     2   492    38     3     6    71   492 90936    55\n",
      " 10067 47455  2044   552   872   771  1520   829    54   524   120    41\n",
      "     2  1265     1    51    24   464   468   603  4784  8696     1   237\n",
      "   594  5261   260  1435  3258   158   829  1837   108  1265   416    86\n",
      "   143   237  1893   679   978    51    18  1047     4    71   763  1257\n",
      "   218    26    25   679  1766   155  2829   542    14     2     1 28304\n",
      "  3807    12    30     3    31    84   889  1823   560   263    58   258\n",
      "     1  2322   704     1    18   165   720    45    21   190   971  2181\n",
      "   224   622   515    42   418   542  3125     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0] INFJ\n"
     ]
    }
   ],
   "source": [
    "# Oversampling training test \n",
    "sm = SMOTE(random_state=42)\n",
    "X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "print('Resampled dataset shape %s' % Counter(y_train))\n",
    "print('example of resampled dataset', X_train[0], y_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encode the target variable\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)\n",
    "\n",
    "\n",
    "\n",
    "#one hot encode labels:\n",
    "\n",
    "# y_train = to_categorical(y_train)\n",
    "# y_test = to_categorical(y_test)\n",
    "\n",
    "# #print example : \n",
    "# print('example of one hot encoded label', y_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "610/610 [==============================] - 2352s 4s/step - loss: -1056.2153 - accuracy: 0.0594\n",
      "Epoch 2/10\n",
      "610/610 [==============================] - 2365s 4s/step - loss: -1797.5033 - accuracy: 0.0594\n",
      "Epoch 3/10\n",
      "610/610 [==============================] - 2447s 4s/step - loss: -2554.4172 - accuracy: 0.0594\n",
      "Epoch 4/10\n",
      "610/610 [==============================] - 2204s 4s/step - loss: -3314.4966 - accuracy: 0.0594\n",
      "Epoch 5/10\n",
      "353/610 [================>.............] - ETA: 15:24 - loss: -3937.3650 - accuracy: 0.0604"
     ]
    }
   ],
   "source": [
    "k_fold = KFold(n_splits=6)\n",
    "scores_k = []\n",
    "confusion_k = np.array([[0, 0], [0, 0]])\n",
    "for train_indices, test_indices in k_fold.split(X_train):\n",
    "    x_train_k = X_train[train_indices]\n",
    "    y_train_k = y_train[train_indices]\n",
    "    x_test_k = X_train[test_indices]\n",
    "    y_test_k = y_train[test_indices]\n",
    "\n",
    "    model.fit(\n",
    "        x_train_k,\n",
    "        y_train_k,\n",
    "        epochs = NUM_EPOCHS,\n",
    "        batch_size = BATCH_SIZE,\n",
    "    )\n",
    "\n",
    "    predictions_k = model.predict(x_test_k) \n",
    "    confusion_k += confusion_matrix(y_test_k, predictions_k)\n",
    "    score_k = accuracy_score(y_test_k, predictions_k)\n",
    "    scores_k.append(score_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Test set classification (individual posts)\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "confusion = confusion_matrix(y_test, predictions)\n",
    "score = accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
