{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/salmazainana/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Embedding, SpatialDropout1D, LSTM, GRU, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "# import LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "#test train split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['type', 'posts', 'cleaned_posts'], dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('cleaned_dataset.csv')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_plots(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.plot(history.history['val_'+string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.legend([string, 'val_'+string])\n",
    "  plt.savefig(string + '.png')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(df['cleaned_posts'])\n",
    "tokenizer.word_index # Get our learned vocabulary\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "VOCAB_SIZE = len(word_index)+1 # Total words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tokenizer.texts_to_sequences(df['cleaned_posts'])\n",
    "MAX_SEQ_LENGTH = max(len(seq) for seq in X)\n",
    "X = tf.keras.preprocessing.sequence.pad_sequences(X, maxlen= MAX_SEQ_LENGTH, padding = 'post') # Pad the sequence to the same length to make it uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "gn_vec_zip_path =\"/Users/salmazainana/Downloads/GoogleNews-vectors-negative300.bin\"\n",
    "model = KeyedVectors.load_word2vec_format(gn_vec_zip_path, binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 300\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, embedding_size))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in word2vec_model:  # Use word2vec_model directly instead of word2vec_model.wv\n",
    "        embedding_vector = word2vec_model[word]  # Same here\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df['type']\n",
    "#one hote encode the target variable such that ENTJ = [1,0,0,0], INTJ = [0,0,0,0], ENTP = [1,0,0,1], ect \n",
    "#dichotomy classification\n",
    "# strip letter from the type so ['ENTJ'] becomes ['E','N','T','J']\n",
    "y_stripped = y.apply(lambda x: list(x.strip()))\n",
    "\n",
    "def preprocessing(y):\n",
    "    if y[0] == 'E':\n",
    "        y[0] = 1\n",
    "    else:\n",
    "        y[0] = 0\n",
    "    if y[1] == 'N':\n",
    "        y[1] = 1\n",
    "    else:\n",
    "        y[1] = 0\n",
    "    if y[2] == 'T':\n",
    "        y[2] = 1\n",
    "    else:\n",
    "        y[2] = 0\n",
    "    if y[3] == 'J':\n",
    "        y[3] = 1\n",
    "    else:\n",
    "        y[3] = 0\n",
    "\n",
    "    return y \n",
    "\n",
    "y = y_stripped.apply(preprocessing)\n",
    "y = np.array(y.tolist())\n",
    "type(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = \"/Users/zainanasalma/Desktop/stanford/courses/CS229/mbti-cs229/Word2vec_model.model\"\n",
    "# #load model from model_path\n",
    "# word2vec_model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "embedding_size = 300\n",
    "# embedding_matrix = np.zeros((VOCAB_SIZE, embedding_size))\n",
    "\n",
    "# for word, i in tokenizer.word_index.items():\n",
    "#     if word in word2vec_model.wv:\n",
    "#         embedding_vector = word2vec_model.wv[word]\n",
    "#         if embedding_vector is not None:\n",
    "#             embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 957, 300)          40590900  \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 957, 512)          1140736   \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 957, 512)          0         \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  (None, 957, 512)          1574912   \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 957, 512)          0         \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirecti  (None, 512)               1574912   \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 2052      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 20        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44883532 (171.22 MB)\n",
      "Trainable params: 4292632 (16.38 MB)\n",
      "Non-trainable params: 40590900 (154.84 MB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      " 2/44 [>.............................] - ETA: 42:06 - loss: 0.3058 - accuracy: 0.3594"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Assuming y_train is a numpy array of integer labels\n",
    "# Convert integer labels to one-hot encoded labels\n",
    "num_classes = 4  # Adjust based on your actual number of classes\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=VOCAB_SIZE, output_dim=embedding_size, weights=[embedding_matrix], input_length=MAX_SEQ_LENGTH, trainable=False))\n",
    "\n",
    "model.add(Bidirectional(LSTM(256, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Bidirectional(LSTM(256, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Bidirectional(LSTM(256, dropout=0.2, recurrent_dropout=0.2, return_sequences=False)))  # Ensure the last LSTM layer does not return sequences\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(4, activation='relu'))  # Intermediate layer (if needed)\n",
    "model.add(Dense(num_classes, activation='softmax'))  # Output layer for 4 classes\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Fit the model\n",
    "# Replace y_train with y_train_one_hot in the model.fit call\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=30, batch_size=128, callbacks=[EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "n_classes = y_test.shape[1]  # Make sure n_classes is defined\n",
    "\n",
    "# Calculate AUC for each binary classification task and plot ROC curves\n",
    "auc_scores = []\n",
    "\n",
    "for i in range(n_classes):\n",
    "    score = roc_auc_score(y_test[:, i], y_pred[:, i])  # Use a different variable name\n",
    "    auc_scores.append(score)\n",
    "    print(f\"AUC for task {i}: {score}\")\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test[:, i], y_pred[:, i])\n",
    "    roc_auc = auc(fpr, tpr)  \n",
    "    plt.plot(fpr, tpr, label=f'Task {i+1} (area = {roc_auc:.2f})')\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random chance')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curves')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate the average AUC across all tasks\n",
    "average_auc = np.mean(auc_scores)\n",
    "print(f\"Average AUC: {average_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_threshold(y_true, y_scores):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "    # Calculate the J statistic for each threshold\n",
    "    J = tpr - fpr\n",
    "    # Find the index of the maximum J statistic\n",
    "    index_of_max_J = np.argmax(J)\n",
    "    best_threshold = thresholds[index_of_max_J]\n",
    "    return best_threshold, J[index_of_max_J]\n",
    "\n",
    "best_threshold= np.zeros(n_classes)\n",
    "for i in range(n_classes):\n",
    "    best_threshold[i], max_J = find_best_threshold(y_test[:, i], y_pred[:, i])\n",
    "    print(f\"Best threshold for task {i}: {best_threshold} with J statistic: {max_J}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#score accuracy\n",
    "for i in range(n_classes):\n",
    "    # return 1 if y_pred[i] > best_threshold[i], else 0\n",
    "    if i == 2: \n",
    "        y_pred[:, i] = (y_pred[:, i] < best_threshold[i]).astype(int)\n",
    "    else :\n",
    "        y_pred[:, i] = (y_pred[:, i] > best_threshold[i]).astype(int)\n",
    "    \n",
    "    print(f\"Accuracy for task {i}: {accuracy_score(y_test[:, i], y_pred[:, i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
